{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\Anaconda3\\envs\\GPT4TS\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import argparse\n",
    "from exp.exp_short_term_forecasting import Exp_Short_Term_Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_model_path = \"..\\..\\huggingface\\gpt2\"\n",
    "# model = GPT2Model.from_pretrained(local_model_path, output_attentions=True, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='gpt4ts')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--task_name', type=str, default='short_term_forecast',\n",
    "                    help='task name, options:[long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]')\n",
    "parser.add_argument('--is_training', type=int, default=0, help='status')\n",
    "parser.add_argument('--model_id', type=str, default='m4_Monthly', help='model id')\n",
    "parser.add_argument('--model', type=str, default='GPT4TS',\n",
    "                    help='model name, options: [Autoformer, Transformer, TimesNet]')\n",
    "parser.add_argument('--local_model_path', type=str, default='..\\..\\huggingface\\gpt2', help='load pretrained model from local folder')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, default='m4', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='../dataset/m4', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "parser.add_argument('--seasonal_patterns', type=str, default='Monthly', help='subset for M4')\n",
    "\n",
    "# inputation task\n",
    "parser.add_argument('--mask_rate', type=float, default=0.25, help='mask ratio')\n",
    "\n",
    "# anomaly detection task\n",
    "parser.add_argument('--anomaly_ratio', type=float, default=0.25, help='prior anomaly ratio (%)')\n",
    "\n",
    "# model define\n",
    "parser.add_argument('--top_k', type=int, default=5, help='for TimesBlock')\n",
    "parser.add_argument('--num_kernels', type=int, default=6, help='for Inception')\n",
    "parser.add_argument('--enc_in', type=int, default=1, help='encoder input size')\n",
    "parser.add_argument('--dec_in', type=int, default=1, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=1, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=128, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=128, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.1, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=1, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=16, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.002, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='Exp', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='SMAPE', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=False, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "\n",
    "# de-stationary projector params\n",
    "parser.add_argument('--p_hidden_dims', type=int, nargs='+', default=[128, 128],\n",
    "                    help='hidden layer dimensions of projector (List)')\n",
    "parser.add_argument('--p_hidden_layers', type=int, default=2, help='number of hidden layers in projector')\n",
    "\n",
    "# patching\n",
    "parser.add_argument('--patch_size', type=int, default=1)\n",
    "parser.add_argument('--stride', type=int, default=1)\n",
    "parser.add_argument('--gpt_layers', type=int, default=6)\n",
    "parser.add_argument('--ln', type=int, default=0)\n",
    "parser.add_argument('--mlp', type=int, default=0)\n",
    "parser.add_argument('--weight', type=float, default=0)\n",
    "parser.add_argument('--percent', type=int, default=5)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.task_name,\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "else:\n",
    "    ii = 0\n",
    "    setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.distil,\n",
    "        args.des, ii)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and test by the original classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp = Exp_Short_Term_Forecast(args)\n",
    "# # exp.model = model\n",
    "# exp.train(setting)\n",
    "# exp.test(setting, test=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GPT4TS\n",
    "from data_provider.m4 import M4Meta\n",
    "from torch import nn\n",
    "from data_provider.data_factory import data_provider\n",
    "from utils.losses import mape_loss, mase_loss, smape_loss\n",
    "import os\n",
    "import time\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate, visual\n",
    "from torch import optim\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.m4_summary import M4Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.data == 'm4':\n",
    "    args.pred_len = M4Meta.horizons_map[args.seasonal_patterns]  # Up to M4 config\n",
    "    args.seq_len = 2 * args.pred_len  # input_len = 2*pred_len\n",
    "    args.label_len = args.pred_len\n",
    "    args.frequency_map = M4Meta.frequency_map[args.seasonal_patterns]\n",
    "model = GPT4TS.Model(args).float()\n",
    "if args.use_multi_gpu and args.use_gpu:\n",
    "    model = nn.DataParallel(model, device_ids=args.device_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loading, take M4 as the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values = np.load(\"../dataset/m4/training.npz\", allow_pickle=True)\n",
    "# m4_info = pd.read_csv(\"../dataset/m4/M4-info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m4_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m4_info.SP.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values[m4_info.SP.values == 'Monthly'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_values = np.array(\n",
    "#     [v[~np.isnan(v)] for v in\n",
    "#         values[m4_info.SP.values == 'Monthly']])  # split different frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 48000\n",
      "val 48000\n",
      "test 48000\n"
     ]
    }
   ],
   "source": [
    "train_data, train_loader = data_provider(args, 'train')\n",
    "vali_data, vali_loader = data_provider(args, 'val')\n",
    "test_data, test_loader = data_provider(args, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trainning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n"
     ]
    }
   ],
   "source": [
    "def select_criterion(loss_name='MSE'):\n",
    "    if loss_name == 'MSE':\n",
    "        return nn.MSELoss()\n",
    "    elif loss_name == 'MAPE':\n",
    "        return mape_loss()\n",
    "    elif loss_name == 'MASE':\n",
    "        return mase_loss()\n",
    "    elif loss_name == 'SMAPE':\n",
    "        return smape_loss()\n",
    "    \n",
    "if args.use_gpu:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu) if not args.use_multi_gpu else args.devices\n",
    "    device = torch.device('cuda:{}'.format(args.gpu))\n",
    "    print('Use GPU: cuda:{}'.format(args.gpu))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Use CPU')\n",
    "\n",
    "path = os.path.join(args.checkpoints, setting)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "train_steps = len(train_loader)\n",
    "early_stopping = EarlyStopping(patience=args.patience, verbose=True)\n",
    "\n",
    "model_optim = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "criterion = select_criterion(args.loss)\n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vali(args, model, train_loader, vali_loader, criterion):\n",
    "    x, _ = train_loader.dataset.last_insample_window()\n",
    "    y = vali_loader.dataset.timeseries\n",
    "    x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "    x = x.unsqueeze(-1)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # decoder input\n",
    "        B, _, C = x.shape\n",
    "        dec_inp = torch.zeros((B, args.pred_len, C)).float().to(device)\n",
    "        dec_inp = torch.cat([x[:, -args.label_len:, :], dec_inp], dim=1).float()\n",
    "        # encoder - decoder\n",
    "        outputs = torch.zeros((B, args.pred_len, C)).float()  # .to(self.device)\n",
    "        id_list = np.arange(0, B, 500)  # validation set size\n",
    "        id_list = np.append(id_list, B)\n",
    "        for i in range(len(id_list) - 1):\n",
    "            outputs[id_list[i]:id_list[i + 1], :, :] = model(x[id_list[i]:id_list[i + 1]], None,\n",
    "                                                                    dec_inp[id_list[i]:id_list[i + 1]],\n",
    "                                                                    None).detach().cpu()\n",
    "        f_dim = -1 if args.features == 'MS' else 0\n",
    "        outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "        pred = outputs\n",
    "        true = torch.from_numpy(np.array(y))\n",
    "        batch_y_mark = torch.ones(true.shape)\n",
    "\n",
    "        loss = criterion(x.detach().cpu()[:, :, 0], args.frequency_map, pred[:, :, 0], true, batch_y_mark)\n",
    "\n",
    "    model.train()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "time_now = time.time()\n",
    "for epoch in range(args.train_epochs):\n",
    "    iter_count = 0\n",
    "    train_loss = []\n",
    "\n",
    "    epoch_time = time.time()\n",
    "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader): # i is the batch id, x is the inputs, y is the labels\n",
    "        iter_count += 1\n",
    "        model_optim.zero_grad()\n",
    "        batch_x = batch_x.float().to(device) # the inputs should be switched to float before inputting into model #[batch_size, sequence len 36, 1]\n",
    "\n",
    "        batch_y = batch_y.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "        # decoder input\n",
    "        dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()\n",
    "        dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device) #[batch_size, 36, 1]\n",
    "\n",
    "        print(\"\\tencoder输入batch的维度为{}\".format(batch_x.shape))\n",
    "\n",
    "        outputs = model(batch_x, None, dec_inp, None)\n",
    "        print(\"\\tmodel输出的维度为{}\".format(outputs.shape))\n",
    "\n",
    "        f_dim = -1 if args.features == 'MS' else 0\n",
    "        outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "        batch_y = batch_y[:, -args.pred_len:, f_dim:].to(device)\n",
    "\n",
    "        batch_y_mark = batch_y_mark[:, -args.pred_len:, f_dim:].to(device)\n",
    "        loss_value = criterion(batch_x, args.frequency_map, outputs, batch_y, batch_y_mark)\n",
    "        loss_sharpness = mse((outputs[:, 1:, :] - outputs[:, :-1, :]), (batch_y[:, 1:, :] - batch_y[:, :-1, :]))\n",
    "        loss = loss_value  # + loss_sharpness * 1e-5\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "            speed = (time.time() - time_now) / iter_count\n",
    "            left_time = speed * ((args.train_epochs - epoch) * train_steps - i)\n",
    "            print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "            iter_count = 0\n",
    "            time_now = time.time()\n",
    "\n",
    "        loss.backward()\n",
    "        model_optim.step()\n",
    "\n",
    "    print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "    train_loss = np.average(train_loss)\n",
    "    vali_loss = vali(args, model, train_loader, vali_loader, criterion)\n",
    "    test_loss = vali_loss\n",
    "    print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "        epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "    early_stopping(vali_loss, model, path) # save model parameters to path\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    adjust_learning_rate(model_optim, epoch + 1, args)\n",
    "\n",
    "# best_model_path = path + '/' + 'checkpoint.pth'\n",
    "# model.load_state_dict(torch.load(best_model_path)) # load model from saved checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = train_loader.dataset.last_insample_window()\n",
    "y = test_loader.dataset.timeseries\n",
    "x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "x = x.unsqueeze(-1)\n",
    "\n",
    "print('loading model')\n",
    "model.load_state_dict(torch.load(os.path.join('./checkpoints/' + setting, 'checkpoint.pth')))\n",
    "\n",
    "folder_path = './test_results/' + setting + '/'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    B, _, C = x.shape\n",
    "    dec_inp = torch.zeros((B, args.pred_len, C)).float().to(device)\n",
    "    dec_inp = torch.cat([x[:, -args.label_len:, :], dec_inp], dim=1).float()\n",
    "    # encoder - decoder\n",
    "    outputs = torch.zeros((B, args.pred_len, C)).float().to(device)\n",
    "    id_list = np.arange(0, B, 1)\n",
    "    id_list = np.append(id_list, B)\n",
    "    for i in range(len(id_list) - 1):\n",
    "        outputs[id_list[i]:id_list[i + 1], :, :] = model(x[id_list[i]:id_list[i + 1]], None,\n",
    "                                                                dec_inp[id_list[i]:id_list[i + 1]], None)\n",
    "\n",
    "        if id_list[i] % 1000 == 0:\n",
    "            print(id_list[i])\n",
    "\n",
    "    f_dim = -1 if args.features == 'MS' else 0\n",
    "    outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "\n",
    "    preds = outputs\n",
    "    trues = y\n",
    "    x = x.detach().cpu().numpy()\n",
    "\n",
    "    for i in range(0, preds.shape[0], preds.shape[0] // 10):\n",
    "        gt = np.concatenate((x[i, :, 0], trues[i]), axis=0)\n",
    "        pd = np.concatenate((x[i, :, 0], preds[i, :, 0]), axis=0)\n",
    "        visual(gt, pd, os.path.join(folder_path, str(i) + '.pdf'))\n",
    "\n",
    "print('test shape:', preds.shape)\n",
    "\n",
    "# result save\n",
    "folder_path = './m4_results/' + args.model + '/'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "forecasts_df = pd.DataFrame(preds[:, :, 0], columns=[f'V{i + 1}' for i in range(args.pred_len)])\n",
    "forecasts_df.index = test_loader.dataset.ids[:preds.shape[0]]\n",
    "forecasts_df.index.name = 'id'\n",
    "forecasts_df.set_index(forecasts_df.columns[0], inplace=True)\n",
    "forecasts_df.to_csv(folder_path + args.seasonal_patterns + '_forecast.csv')\n",
    "\n",
    "print(args.model)\n",
    "file_path = './m4_results/' + args.model + '/'\n",
    "if 'Weekly_forecast.csv' in os.listdir(file_path) \\\n",
    "        and 'Monthly_forecast.csv' in os.listdir(file_path) \\\n",
    "        and 'Yearly_forecast.csv' in os.listdir(file_path) \\\n",
    "        and 'Daily_forecast.csv' in os.listdir(file_path) \\\n",
    "        and 'Hourly_forecast.csv' in os.listdir(file_path) \\\n",
    "        and 'Quarterly_forecast.csv' in os.listdir(file_path):\n",
    "    m4_summary = M4Summary(file_path, args.root_path)\n",
    "    # m4_forecast.set_index(m4_winner_forecast.columns[0], inplace=True)\n",
    "    smape_results, owa_results, mape, mase = m4_summary.evaluate()\n",
    "    print('smape:', smape_results)\n",
    "    print('mape:', mape)\n",
    "    print('mase:', mase)\n",
    "    print('owa:', owa_results)\n",
    "else:\n",
    "    print('After all 6 tasks are finished, you can calculate the averaged index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# local_model_path = \"..\\..\\huggingface\\gpt2\"\n",
    "# model_gpt2 = GPT2LMHeadModel.from_pretrained(local_model_path, output_attentions=True, output_hidden_states=True)\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "# text = \"what did you do last weekend?\"\n",
    "\n",
    "# # 编码输入文本\n",
    "# input_ids = tokenizer.encode(text, return_tensors='pt') # [1, 7]\n",
    "\n",
    "# # 生成文本\n",
    "# output = model_gpt2.generate(input_ids, max_length=100, num_return_sequences=1) # [1,100]\n",
    "\n",
    "\n",
    "# # 解码生成的文本\n",
    "# generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# # 打印生成的文本\n",
    "# print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT4TS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
